{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from numpy import expand_dims\n",
        "from numpy import zeros\n",
        "from numpy import ones\n",
        "from numpy.random import randn\n",
        "from numpy.random import randint\n",
        "from keras.datasets.fashion_mnist import load_data\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import Flatten, MaxPooling2D\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Conv2DTranspose, UpSampling2D\n",
        "from keras.layers import LeakyReLU, ReLU\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Concatenate\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from PIL import Image, ImageOps\n",
        "import os\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1620721764632
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = '/mnt/batch/tasks/shared/LS_root/mounts/clusters/cpacganfirstattempt/code/Users/10539060'\n",
        "image_size = 256\n",
        "\n",
        "# insert images as np.array\n",
        "training_images_path = '/mnt/batch/tasks/shared/LS_root/mounts/clusters/cpacganfirstattempt/code/Users/10539060/images_256_256_MD.npy'\n",
        "\n",
        "# insert classes as np.array Users/10539060/images_256_256_MD.npy\n",
        "training_labels_path = '/mnt/batch/tasks/shared/LS_root/mounts/clusters/cpacganfirstattempt/code/Users/10539060/labels_256_256_MD.npy'"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1620721764798
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the standalone discriminator model\n",
        "def define_discriminator(in_shape=(256,256,3), n_classes=4):\n",
        "        # label input\n",
        "        in_label = Input(shape=(1,))\n",
        "        # embedding for categorical input\n",
        "        li = Embedding(n_classes, 35)(in_label)\n",
        "        # scale up to image dimensions with linear activation\n",
        "        n_nodes = in_shape[0] * in_shape[1]\n",
        "        li = Dense(n_nodes)(li)\n",
        "        # reshape to additional channel\n",
        "        li = Reshape((in_shape[0], in_shape[1], 1))(li)\n",
        "        # image input\n",
        "        in_image = Input(shape=in_shape)\n",
        "        # concat label as a channel\n",
        "        merge = Concatenate()([in_image, li])\n",
        "        fe = Conv2D(256, (3,3), strides=(2,2), padding='same')(merge)\n",
        "        fe = LeakyReLU(alpha=0.2)(fe) \n",
        "        # downsample\n",
        "        fe = Conv2D(512, (3,3), strides=(2,2), padding='same')(fe)\n",
        "        fe = LeakyReLU(alpha=0.2)(fe)\n",
        "        # flatten feature maps\n",
        "        fe = Flatten()(fe)\n",
        "        # dropout\n",
        "        fe = Dropout(0.4)(fe)\n",
        "        # output\n",
        "        out_layer = Dense(1, activation='sigmoid')(fe)\n",
        "        # define model\n",
        "        model = Model([in_image, in_label], out_layer)\n",
        "        # compile model\n",
        "        opt = Adam(lr=0.0002, beta_1=0.5) \n",
        "        model.summary()\n",
        "        model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "# define the standalone generator model\n",
        "def define_generator(latent_dim, size, n_classes=4):\n",
        "    # label input\n",
        "        in_label = Input(shape=(1,))\n",
        "        # embedding for categorical input\n",
        "        li = Embedding(n_classes, 35)(in_label)\n",
        "        # linear multiplication\n",
        "        n_nodes = int(size/4) * int(size/4)\n",
        "        li = Dense(n_nodes)(li)\n",
        "        # reshape to additional channel\n",
        "        li = Reshape((int(size/4), int(size/4), 1))(li)\n",
        "        # image generator input\n",
        "        in_lat = Input(shape=(latent_dim,))\n",
        "        # foundation\n",
        "        n_nodes = 128 * int(size/4) * int(size/4)\n",
        "        gen = Dense(n_nodes)(in_lat)\n",
        "        gen = LeakyReLU(alpha=0.2)(gen)\n",
        "        gen = Reshape((int(size/4), int(size/4), 128))(gen)\n",
        "        # merge image gen and label input\n",
        "        merge = Concatenate()([gen, li])\n",
        "        # first upsample\n",
        "        gen = Conv2DTranspose(512, (3,3), strides=(2, 2), padding='same')(merge)  \n",
        "        gen = LeakyReLU(alpha=0.3)(gen)\n",
        "        #gen = UpSampling2D()(gen)\n",
        "        # second upsample\n",
        "        gen = Conv2DTranspose(256, (3,3), strides=(2, 2), padding='same')(gen) \n",
        "        gen = LeakyReLU(alpha=0.3)(gen)\n",
        "        #gen = UpSampling2D()(gen)\n",
        "        # output\n",
        "        out_layer = Conv2D(3, (7,7), activation='tanh', padding='same')(gen)\n",
        "        # define model\n",
        "        model = Model([in_lat, in_label], out_layer)\n",
        "        model.summary()\n",
        "        return model\n",
        "\n",
        "# define the combined generator and discriminator model, for updating the generator\n",
        "def define_gan(g_model, d_model):\n",
        "\t# make weights in the discriminator not trainable\n",
        "\td_model.trainable = False\n",
        "\t# get noise and label inputs from generator model\n",
        "\tgen_noise, gen_label= g_model.input\n",
        "\t# get image output from the generator model\n",
        "\tgen_output = g_model.output\n",
        "\t# connect image output and label input from generator as inputs to discriminator\n",
        "\tgan_output = d_model([gen_output, gen_label])\n",
        "\t# define gan model as taking noise and label and outputting a classification\n",
        "\tmodel = Model([gen_noise, gen_label], gan_output)\n",
        "\t# compile model\n",
        "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=opt)\n",
        "\treturn model"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1620721953891
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_real_samples():\n",
        "  trainX = np.load(training_images_path, allow_pickle = True)\n",
        "  trainy = np.load(training_labels_path, allow_pickle = True)\n",
        "  #Adding noise to input data\n",
        "  X = trainX.astype('float32')\n",
        "  X = (X - 127.5) / 127.5 + np.random.normal(0, 0.01, (trainX.shape[0],trainX.shape[1],trainX.shape[2],trainX.shape[3]))\n",
        "  #From str labels to integers\n",
        "  for i in range(len(trainy)):\n",
        "    if trainy[i] == 'Happy':\n",
        "      trainy[i] = np.int(0)\n",
        "    if trainy[i] == 'Angry':\n",
        "      trainy[i] = np.int(1)\n",
        "    if trainy[i] == 'Sad':\n",
        "      trainy[i] = np.int(2)\n",
        "    if trainy[i] == 'Relaxed':\n",
        "      trainy[i] = np.int(3)\n",
        "  trainy = trainy.astype('float32')\n",
        "  return[X, trainy]"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1620721961487
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # select real samples\n",
        "def generate_real_samples(dataset, n_samples):\n",
        "  # split into images and labels\n",
        "  images, labels = dataset\n",
        "  # choose random instances\n",
        "  ix = randint(0, images.shape[0], n_samples)\n",
        "  # select images and labels\n",
        "  X, labels = images[ix], labels[ix]\n",
        "  # generate class labels\n",
        "  y = ones((n_samples, 1))\n",
        "  # labels switching\n",
        "  for i in range(int(n_samples/3)):\n",
        "    y[3*i] = 0 \n",
        "  return [X, labels], y"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1620721964070
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate points in latent space as input for the generator\n",
        "def generate_latent_points(latent_dim, n_samples, n_classes=4):\n",
        "\t# generate points in the latent space\n",
        "\tx_input = randn(latent_dim * n_samples)\n",
        "\t# reshape into a batch of inputs for the network\n",
        "\tz_input = x_input.reshape(n_samples, latent_dim)\n",
        "\t# generate labels\n",
        "\tlabels = randint(0, n_classes, n_samples)\n",
        "\treturn [z_input, labels]"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1620721966738
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use the generator to generate n fake examples, with class labels\n",
        "def generate_fake_samples(generator, latent_dim, n_samples):\n",
        "  # generate points in latent space\n",
        "  z_input, labels_input = generate_latent_points(latent_dim, n_samples)\n",
        "  # predict outputs\n",
        "  images = generator.predict([z_input, labels_input])\n",
        "  # create class labels\n",
        "  y = zeros((n_samples, 1))\n",
        "  #labels switching\n",
        "  for i in range(int(n_samples/3)):\n",
        "    y[3*i] = 1\n",
        "  return [images, labels_input], y"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1620721969473
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train the generator and discriminator\n",
        "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=25, n_batch=16): #batch era 128\n",
        "  bat_per_epo = int(dataset[0].shape[0] / n_batch)\n",
        "  half_batch = int(n_batch / 2)\n",
        "  # manually enumerate epochs\n",
        "  for i in range(n_epochs):\n",
        "    # enumerate batches over the training set\n",
        "    for j in range(bat_per_epo):\n",
        "      # get randomly selected 'real' samples\n",
        "      [X_real, labels_real], y_real = generate_real_samples(dataset, half_batch)\n",
        "      # update discriminator model weights\n",
        "      d_loss1, _ = d_model.train_on_batch([X_real, labels_real], y_real)\n",
        "      # generate 'fake' examples\n",
        "      [X_fake, labels], y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
        "      # update discriminator model weights\n",
        "      d_loss2, _ = d_model.train_on_batch([X_fake, labels], y_fake)\n",
        "      # prepare points in latent space as input for the generator\n",
        "      [z_input, labels_input] = generate_latent_points(latent_dim, n_batch)\n",
        "      # create inverted labels for the fake samples\n",
        "      y_gan = ones((n_batch, 1))\n",
        "      # update the generator via the discriminator's error\n",
        "      g_loss = gan_model.train_on_batch([z_input, labels_input], y_gan)\n",
        "      # summarize loss on this batch\n",
        "      print('>%d, %d/%d, d1=%.3f, d2=%.3f g=%.3f' %\n",
        "        (i+1, j+1, bat_per_epo, d_loss1, d_loss2, g_loss))"
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1620721971666
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# size of the latent space\n",
        "latent_dim = 16\n",
        "# create the discriminator\n",
        "d_model = define_discriminator()\n",
        "# create the generator\n",
        "g_model = define_generator(latent_dim, image_size)\n",
        "# create the gan\n",
        "gan_model = define_gan(g_model, d_model)\n",
        "# load image data\n",
        "dataset = load_real_samples()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (None, 1, 35)        140         input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 1, 65536)     2359296     embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "input_6 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "reshape_4 (Reshape)             (None, 256, 256, 1)  0           dense_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 256, 256, 4)  0           input_6[0][0]                    \n",
            "                                                                 reshape_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 128, 128, 256 9472        concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)       (None, 128, 128, 256 0           conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 64, 64, 512)  1180160     leaky_re_lu_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)       (None, 64, 64, 512)  0           conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 2097152)      0           leaky_re_lu_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 2097152)      0           flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 1)            2097153     dropout_2[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 5,646,221\n",
            "Trainable params: 5,646,221\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_8 (InputLayer)            (None, 16)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_7 (InputLayer)            (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 524288)       8912896     input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_4 (Embedding)         (None, 1, 35)        140         input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)       (None, 524288)       0           dense_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 1, 4096)      147456      embedding_4[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "reshape_6 (Reshape)             (None, 64, 64, 128)  0           leaky_re_lu_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "reshape_5 (Reshape)             (None, 64, 64, 1)    0           dense_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 64, 64, 129)  0           reshape_6[0][0]                  \n",
            "                                                                 reshape_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_3 (Conv2DTrans (None, 128, 128, 512 594944      concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)       (None, 128, 128, 512 0           conv2d_transpose_3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_4 (Conv2DTrans (None, 256, 256, 256 1179904     leaky_re_lu_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_10 (LeakyReLU)      (None, 256, 256, 256 0           conv2d_transpose_4[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 256, 256, 3)  37635       leaky_re_lu_10[0][0]             \n",
            "==================================================================================================\n",
            "Total params: 10,872,975\n",
            "Trainable params: 10,872,975\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "execution_count": 17,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1620722074888
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train and save model\n",
        "train(g_model, d_model, gan_model, dataset, latent_dim)\n",
        "g_model.save(os.path.join(DATA_PATH, f'generator_{image_size}_100_new.h5'))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n",
            "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n",
            "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">1, 1/465, d1=0.701, d2=0.697 g=0.707\n",
            ">1, 2/465, d1=1.788, d2=0.683 g=0.728\n",
            ">1, 3/465, d1=0.388, d2=0.684 g=0.724\n",
            ">1, 4/465, d1=1.006, d2=0.684 g=0.724\n",
            ">1, 5/465, d1=1.116, d2=0.681 g=0.726\n",
            ">1, 6/465, d1=1.119, d2=0.688 g=0.716\n",
            ">1, 7/465, d1=0.722, d2=0.682 g=0.761\n",
            ">1, 8/465, d1=1.064, d2=0.668 g=0.780\n",
            ">1, 9/465, d1=0.650, d2=0.658 g=0.811\n",
            ">1, 10/465, d1=0.665, d2=0.651 g=0.845\n",
            ">1, 11/465, d1=0.675, d2=0.640 g=0.915\n",
            ">1, 12/465, d1=1.366, d2=0.662 g=0.785\n",
            ">1, 13/465, d1=0.500, d2=0.677 g=0.865\n",
            ">1, 14/465, d1=0.875, d2=0.589 g=1.287\n",
            ">1, 15/465, d1=1.316, d2=0.689 g=0.659\n",
            ">1, 16/465, d1=0.732, d2=0.776 g=0.705\n",
            ">1, 17/465, d1=0.683, d2=0.755 g=0.826\n",
            ">1, 18/465, d1=0.664, d2=0.724 g=0.813\n",
            ">1, 19/465, d1=0.792, d2=0.670 g=1.075\n",
            ">1, 20/465, d1=0.656, d2=0.676 g=0.961\n",
            ">1, 21/465, d1=1.071, d2=0.616 g=1.151\n",
            ">1, 22/465, d1=0.645, d2=0.644 g=0.990\n",
            ">1, 23/465, d1=1.655, d2=0.648 g=0.982\n",
            ">1, 24/465, d1=1.112, d2=0.680 g=0.824\n",
            ">1, 25/465, d1=0.785, d2=0.695 g=0.805\n",
            ">1, 26/465, d1=0.551, d2=0.627 g=1.162\n",
            ">1, 27/465, d1=0.566, d2=0.601 g=1.182\n",
            ">1, 28/465, d1=0.912, d2=0.768 g=0.929\n",
            ">1, 29/465, d1=0.561, d2=0.561 g=1.606\n",
            ">1, 30/465, d1=0.816, d2=0.582 g=1.180\n",
            ">1, 31/465, d1=0.521, d2=0.571 g=1.363\n",
            ">1, 32/465, d1=0.933, d2=0.578 g=1.218\n",
            ">1, 33/465, d1=0.576, d2=0.604 g=1.303\n",
            ">1, 34/465, d1=0.773, d2=0.579 g=1.277\n",
            ">1, 35/465, d1=0.902, d2=0.638 g=0.802\n",
            ">1, 36/465, d1=0.709, d2=0.664 g=0.920\n",
            ">1, 37/465, d1=0.675, d2=0.656 g=1.238\n",
            ">1, 38/465, d1=0.603, d2=0.594 g=1.655\n",
            ">1, 39/465, d1=1.073, d2=0.663 g=0.672\n",
            ">1, 40/465, d1=1.250, d2=0.680 g=1.221\n",
            ">1, 41/465, d1=0.758, d2=0.564 g=1.211\n",
            ">1, 42/465, d1=0.518, d2=0.573 g=1.425\n",
            ">1, 43/465, d1=0.914, d2=0.591 g=1.228\n",
            ">1, 44/465, d1=0.711, d2=0.611 g=1.275\n",
            ">1, 45/465, d1=0.603, d2=0.583 g=1.191\n",
            ">1, 46/465, d1=0.626, d2=0.541 g=1.768\n",
            ">1, 47/465, d1=0.672, d2=0.555 g=0.956\n",
            ">1, 48/465, d1=0.630, d2=0.660 g=1.588\n",
            ">1, 49/465, d1=0.511, d2=0.552 g=1.322\n",
            ">1, 50/465, d1=0.875, d2=0.615 g=1.820\n",
            ">1, 51/465, d1=1.301, d2=0.578 g=1.347\n",
            ">1, 52/465, d1=0.688, d2=0.589 g=0.994\n",
            ">1, 53/465, d1=0.674, d2=0.633 g=1.611\n",
            ">1, 54/465, d1=0.596, d2=0.534 g=1.493\n",
            ">1, 55/465, d1=0.471, d2=0.677 g=2.659\n",
            ">1, 56/465, d1=1.204, d2=0.498 g=1.314\n",
            ">1, 57/465, d1=0.679, d2=0.660 g=1.826\n",
            ">1, 58/465, d1=0.828, d2=0.527 g=1.644\n",
            ">1, 59/465, d1=0.806, d2=0.717 g=1.144\n",
            ">1, 60/465, d1=0.592, d2=0.631 g=1.395\n",
            ">1, 61/465, d1=0.753, d2=0.577 g=1.763\n",
            ">1, 62/465, d1=0.702, d2=0.643 g=1.468\n",
            ">1, 63/465, d1=0.778, d2=0.590 g=1.381\n",
            ">1, 64/465, d1=1.001, d2=0.841 g=0.917\n",
            ">1, 65/465, d1=0.581, d2=0.566 g=1.648\n",
            ">1, 66/465, d1=0.616, d2=0.591 g=1.081\n",
            ">1, 67/465, d1=0.862, d2=0.698 g=1.912\n",
            ">1, 68/465, d1=0.803, d2=0.588 g=1.179\n",
            ">1, 69/465, d1=0.660, d2=0.693 g=1.118\n",
            ">1, 70/465, d1=0.808, d2=0.661 g=1.315\n",
            ">1, 71/465, d1=0.521, d2=0.552 g=1.456\n",
            ">1, 72/465, d1=0.520, d2=0.792 g=1.893\n",
            ">1, 73/465, d1=0.735, d2=0.629 g=1.429\n",
            ">1, 74/465, d1=1.050, d2=0.740 g=1.160\n",
            ">1, 75/465, d1=0.665, d2=0.558 g=1.454\n",
            ">1, 76/465, d1=0.862, d2=0.608 g=1.134\n",
            ">1, 77/465, d1=0.811, d2=0.537 g=1.289\n",
            ">1, 78/465, d1=0.572, d2=0.596 g=1.463\n",
            ">1, 79/465, d1=0.463, d2=0.568 g=1.407\n",
            ">1, 80/465, d1=0.821, d2=0.549 g=1.284\n",
            ">1, 81/465, d1=0.865, d2=0.612 g=2.188\n",
            ">1, 82/465, d1=0.577, d2=0.637 g=1.256\n",
            ">1, 83/465, d1=0.611, d2=0.915 g=2.395\n",
            ">1, 84/465, d1=0.652, d2=0.846 g=1.614\n",
            ">1, 85/465, d1=0.574, d2=0.650 g=1.281\n",
            ">1, 86/465, d1=0.762, d2=0.559 g=1.512\n",
            ">1, 87/465, d1=0.659, d2=0.575 g=1.327\n",
            ">1, 88/465, d1=0.550, d2=0.696 g=1.685\n",
            ">1, 89/465, d1=0.736, d2=0.631 g=1.302\n",
            ">1, 90/465, d1=0.852, d2=0.584 g=1.216\n",
            ">1, 91/465, d1=0.585, d2=0.606 g=1.249\n",
            ">1, 92/465, d1=0.438, d2=0.602 g=1.518\n",
            ">1, 93/465, d1=0.719, d2=0.661 g=1.074\n",
            ">1, 94/465, d1=0.677, d2=0.662 g=1.023\n",
            ">1, 95/465, d1=0.746, d2=0.918 g=0.888\n",
            ">1, 96/465, d1=0.495, d2=0.608 g=1.147\n",
            ">1, 97/465, d1=0.423, d2=0.539 g=1.448\n",
            ">1, 98/465, d1=0.751, d2=0.657 g=1.267\n",
            ">1, 99/465, d1=0.647, d2=0.827 g=1.104\n",
            ">1, 100/465, d1=0.620, d2=0.820 g=1.550\n",
            ">1, 101/465, d1=0.514, d2=0.729 g=1.227\n",
            ">1, 102/465, d1=0.626, d2=0.681 g=1.684\n",
            ">1, 103/465, d1=1.193, d2=0.861 g=1.324\n",
            ">1, 104/465, d1=0.884, d2=0.987 g=0.733\n",
            ">1, 105/465, d1=0.508, d2=1.072 g=1.470\n",
            ">1, 106/465, d1=0.670, d2=0.548 g=1.363\n",
            ">1, 107/465, d1=0.633, d2=0.657 g=0.832\n",
            ">1, 108/465, d1=0.846, d2=0.809 g=1.302\n",
            ">1, 109/465, d1=0.660, d2=0.565 g=0.945\n",
            ">1, 110/465, d1=0.707, d2=0.660 g=1.215\n",
            ">1, 111/465, d1=0.702, d2=0.805 g=0.958\n",
            ">1, 112/465, d1=0.780, d2=0.756 g=1.247\n",
            ">1, 113/465, d1=0.828, d2=0.630 g=1.159\n",
            ">1, 114/465, d1=0.567, d2=0.788 g=1.172\n",
            ">1, 115/465, d1=0.633, d2=0.530 g=1.636\n",
            ">1, 116/465, d1=1.017, d2=0.540 g=1.400\n",
            ">1, 117/465, d1=0.895, d2=0.686 g=1.357\n",
            ">1, 118/465, d1=0.586, d2=0.617 g=1.137\n",
            ">1, 119/465, d1=1.104, d2=0.646 g=0.900\n",
            ">1, 120/465, d1=0.647, d2=0.643 g=1.141\n",
            ">1, 121/465, d1=0.708, d2=1.019 g=0.992\n",
            ">1, 122/465, d1=0.594, d2=0.582 g=1.156\n",
            ">1, 123/465, d1=0.654, d2=0.610 g=0.999\n",
            ">1, 124/465, d1=0.546, d2=0.694 g=1.097\n",
            ">1, 125/465, d1=0.875, d2=0.397 g=1.601\n",
            ">1, 126/465, d1=0.547, d2=0.809 g=1.245\n",
            ">1, 127/465, d1=0.585, d2=0.757 g=1.201\n",
            ">1, 128/465, d1=1.475, d2=0.802 g=1.219\n",
            ">1, 129/465, d1=0.832, d2=0.533 g=1.332\n",
            ">1, 130/465, d1=0.767, d2=0.681 g=0.836\n",
            ">1, 131/465, d1=0.678, d2=0.899 g=1.359\n",
            ">1, 132/465, d1=0.679, d2=0.631 g=1.174\n",
            ">1, 133/465, d1=0.477, d2=0.533 g=1.468\n",
            ">1, 134/465, d1=0.660, d2=0.503 g=1.089\n",
            ">1, 135/465, d1=0.829, d2=0.822 g=1.157\n",
            ">1, 136/465, d1=0.537, d2=0.578 g=1.435\n",
            ">1, 137/465, d1=0.639, d2=0.555 g=1.324\n",
            ">1, 138/465, d1=1.056, d2=0.467 g=0.667\n",
            ">1, 139/465, d1=0.805, d2=1.039 g=1.084\n",
            ">1, 140/465, d1=1.258, d2=0.561 g=1.222\n",
            ">1, 141/465, d1=0.736, d2=0.615 g=0.901\n",
            ">1, 142/465, d1=0.639, d2=0.652 g=1.012\n",
            ">1, 143/465, d1=0.700, d2=0.725 g=0.890\n",
            ">1, 144/465, d1=0.625, d2=0.610 g=0.897\n",
            ">1, 145/465, d1=0.815, d2=0.761 g=0.914\n",
            ">1, 146/465, d1=0.586, d2=0.739 g=1.235\n",
            ">1, 147/465, d1=0.750, d2=0.605 g=1.436\n",
            ">1, 148/465, d1=0.697, d2=0.590 g=0.974\n",
            ">1, 149/465, d1=0.784, d2=0.594 g=1.094\n",
            ">1, 150/465, d1=0.631, d2=0.548 g=1.294\n",
            ">1, 151/465, d1=0.705, d2=0.419 g=1.382\n",
            ">1, 152/465, d1=0.992, d2=0.653 g=0.683\n",
            ">1, 153/465, d1=0.613, d2=0.872 g=1.133\n",
            ">1, 154/465, d1=0.633, d2=0.596 g=1.212\n",
            ">1, 155/465, d1=0.927, d2=0.639 g=0.926\n",
            ">1, 156/465, d1=0.644, d2=0.833 g=1.956\n",
            ">1, 157/465, d1=0.873, d2=0.660 g=1.448\n",
            ">1, 158/465, d1=0.780, d2=0.527 g=1.385\n",
            ">1, 159/465, d1=0.647, d2=0.670 g=2.074\n",
            ">1, 160/465, d1=1.032, d2=0.642 g=1.163\n",
            ">1, 161/465, d1=0.710, d2=0.906 g=1.961\n",
            ">1, 162/465, d1=0.592, d2=0.579 g=1.488\n",
            ">1, 163/465, d1=0.470, d2=0.561 g=1.825\n",
            ">1, 164/465, d1=0.705, d2=0.578 g=1.614\n",
            ">1, 165/465, d1=0.613, d2=0.676 g=1.847\n",
            ">1, 166/465, d1=0.794, d2=0.569 g=1.332\n",
            ">1, 167/465, d1=0.587, d2=0.543 g=1.218\n",
            ">1, 168/465, d1=0.727, d2=0.678 g=1.027\n",
            ">1, 169/465, d1=0.619, d2=0.607 g=1.250\n",
            ">1, 170/465, d1=0.989, d2=0.601 g=1.152\n",
            ">1, 171/465, d1=0.730, d2=0.593 g=1.020\n",
            ">1, 172/465, d1=1.218, d2=0.693 g=0.845\n",
            ">1, 173/465, d1=0.554, d2=0.648 g=0.836\n",
            ">1, 174/465, d1=0.616, d2=0.653 g=0.963\n",
            ">1, 175/465, d1=0.623, d2=0.640 g=1.003\n",
            ">1, 176/465, d1=0.741, d2=0.640 g=1.120\n",
            ">1, 177/465, d1=0.544, d2=0.582 g=1.133\n",
            ">1, 178/465, d1=0.569, d2=0.634 g=1.273\n",
            ">1, 179/465, d1=0.905, d2=0.644 g=1.108\n",
            ">1, 180/465, d1=0.486, d2=0.655 g=1.343\n",
            ">1, 181/465, d1=0.950, d2=0.651 g=1.110\n",
            ">1, 182/465, d1=1.030, d2=0.636 g=1.059\n",
            ">1, 183/465, d1=1.006, d2=0.738 g=0.812\n",
            ">1, 184/465, d1=0.655, d2=0.738 g=0.954\n",
            ">1, 185/465, d1=0.855, d2=0.902 g=1.134\n",
            ">1, 186/465, d1=0.743, d2=0.636 g=1.079\n",
            ">1, 187/465, d1=0.619, d2=0.628 g=1.039\n",
            ">1, 188/465, d1=0.692, d2=0.671 g=1.134\n",
            ">1, 189/465, d1=0.572, d2=0.580 g=1.257\n",
            ">1, 190/465, d1=0.443, d2=0.686 g=1.179\n",
            ">1, 191/465, d1=0.665, d2=0.767 g=1.410\n",
            ">1, 192/465, d1=1.074, d2=0.584 g=1.193\n",
            ">1, 193/465, d1=0.522, d2=0.718 g=1.238\n",
            ">1, 194/465, d1=0.534, d2=0.403 g=1.044\n",
            ">1, 195/465, d1=0.900, d2=0.743 g=0.977\n",
            ">1, 196/465, d1=0.847, d2=0.587 g=1.589\n",
            ">1, 197/465, d1=0.862, d2=0.580 g=1.349\n",
            ">1, 198/465, d1=0.856, d2=0.711 g=0.936\n",
            ">1, 199/465, d1=0.634, d2=0.663 g=1.283\n",
            ">1, 200/465, d1=1.063, d2=0.967 g=1.238\n",
            ">1, 201/465, d1=0.749, d2=0.693 g=1.583\n",
            ">1, 202/465, d1=0.778, d2=0.608 g=1.235\n",
            ">1, 203/465, d1=0.605, d2=0.611 g=1.314\n",
            ">1, 204/465, d1=0.647, d2=0.525 g=1.424\n",
            ">1, 205/465, d1=0.463, d2=0.854 g=1.482\n",
            ">1, 206/465, d1=0.690, d2=0.580 g=1.487\n",
            ">1, 207/465, d1=0.695, d2=0.572 g=1.140\n",
            ">1, 208/465, d1=0.438, d2=0.730 g=1.371\n",
            ">1, 209/465, d1=0.510, d2=0.545 g=1.240\n",
            ">1, 210/465, d1=0.551, d2=0.705 g=1.606\n",
            ">1, 211/465, d1=0.767, d2=0.508 g=1.316\n",
            ">1, 212/465, d1=0.724, d2=1.256 g=2.344\n",
            ">1, 213/465, d1=1.391, d2=0.512 g=1.378\n",
            ">1, 214/465, d1=0.622, d2=0.463 g=0.928\n",
            ">1, 215/465, d1=0.705, d2=0.568 g=1.424\n",
            ">1, 216/465, d1=0.587, d2=0.530 g=1.542\n",
            ">1, 217/465, d1=0.810, d2=0.928 g=0.987\n",
            ">1, 218/465, d1=0.902, d2=0.583 g=1.023\n",
            ">1, 219/465, d1=0.671, d2=0.807 g=1.002\n",
            ">1, 220/465, d1=0.662, d2=0.890 g=0.931\n",
            ">1, 221/465, d1=0.637, d2=0.481 g=1.813\n",
            ">1, 222/465, d1=1.167, d2=0.906 g=0.931\n",
            ">1, 223/465, d1=0.452, d2=0.730 g=1.235\n",
            ">1, 224/465, d1=0.592, d2=0.774 g=1.310\n",
            ">1, 225/465, d1=0.474, d2=0.653 g=1.252\n",
            ">1, 226/465, d1=0.688, d2=0.932 g=1.318\n",
            ">1, 227/465, d1=0.788, d2=0.634 g=1.375\n",
            ">1, 228/465, d1=0.677, d2=0.630 g=1.428\n",
            ">1, 229/465, d1=0.567, d2=0.661 g=1.372\n",
            ">1, 230/465, d1=0.804, d2=0.923 g=1.020\n",
            ">1, 231/465, d1=0.930, d2=0.694 g=1.402\n",
            ">1, 232/465, d1=0.757, d2=0.620 g=1.118\n",
            ">1, 233/465, d1=0.712, d2=0.761 g=1.288\n",
            ">1, 234/465, d1=0.730, d2=0.548 g=1.462\n",
            ">1, 235/465, d1=0.688, d2=0.614 g=1.265\n",
            ">1, 236/465, d1=0.749, d2=0.907 g=1.411\n",
            ">1, 237/465, d1=0.889, d2=0.741 g=1.054\n",
            ">1, 238/465, d1=0.505, d2=0.655 g=1.077\n",
            ">1, 239/465, d1=0.631, d2=0.508 g=1.411\n",
            ">1, 240/465, d1=0.806, d2=0.744 g=1.152\n",
            ">1, 241/465, d1=0.814, d2=1.205 g=1.249\n",
            ">1, 242/465, d1=0.683, d2=0.674 g=1.313\n",
            ">1, 243/465, d1=0.648, d2=0.540 g=1.005\n",
            ">1, 244/465, d1=0.932, d2=0.835 g=0.778\n",
            ">1, 245/465, d1=0.672, d2=0.701 g=0.796\n",
            ">1, 246/465, d1=0.711, d2=0.719 g=0.914\n",
            ">1, 247/465, d1=0.570, d2=0.675 g=1.063\n",
            ">1, 248/465, d1=0.744, d2=0.579 g=1.092\n",
            ">1, 249/465, d1=0.598, d2=0.614 g=1.178\n",
            ">1, 250/465, d1=0.321, d2=0.584 g=1.476\n",
            ">1, 251/465, d1=0.415, d2=0.959 g=1.072\n",
            ">1, 252/465, d1=0.547, d2=0.683 g=1.003\n",
            ">1, 253/465, d1=0.714, d2=0.635 g=1.174\n",
            ">1, 254/465, d1=0.534, d2=0.732 g=1.107\n",
            ">1, 255/465, d1=0.558, d2=0.773 g=1.250\n",
            ">1, 256/465, d1=0.698, d2=0.640 g=1.525\n",
            ">1, 257/465, d1=0.719, d2=0.570 g=1.135\n",
            ">1, 258/465, d1=0.632, d2=0.667 g=0.960\n",
            ">1, 259/465, d1=0.909, d2=0.675 g=1.008\n",
            ">1, 260/465, d1=0.822, d2=0.702 g=1.087\n",
            ">1, 261/465, d1=1.026, d2=0.637 g=1.372\n",
            ">1, 262/465, d1=0.568, d2=0.485 g=2.296\n",
            ">1, 263/465, d1=0.816, d2=0.664 g=0.818\n",
            ">1, 264/465, d1=0.621, d2=0.766 g=1.016\n",
            ">1, 265/465, d1=0.749, d2=0.502 g=1.128\n",
            ">1, 266/465, d1=0.735, d2=0.760 g=0.836\n",
            ">1, 267/465, d1=0.639, d2=0.707 g=1.257\n",
            ">1, 268/465, d1=0.817, d2=0.763 g=1.048\n",
            ">1, 269/465, d1=0.679, d2=0.802 g=1.226\n",
            ">1, 270/465, d1=0.737, d2=0.604 g=0.891\n",
            ">1, 271/465, d1=0.496, d2=0.822 g=1.074\n",
            ">1, 272/465, d1=0.747, d2=0.654 g=1.379\n",
            ">1, 273/465, d1=1.132, d2=0.697 g=0.951\n",
            ">1, 274/465, d1=0.544, d2=0.586 g=0.957\n",
            ">1, 275/465, d1=0.601, d2=0.504 g=1.075\n",
            ">1, 276/465, d1=0.536, d2=0.649 g=1.334\n",
            ">1, 277/465, d1=0.727, d2=0.644 g=0.812\n",
            ">1, 278/465, d1=1.162, d2=0.745 g=0.833\n",
            ">1, 279/465, d1=0.643, d2=0.683 g=1.087\n",
            ">1, 280/465, d1=0.515, d2=0.662 g=1.235\n",
            ">1, 281/465, d1=0.612, d2=0.572 g=1.066\n",
            ">1, 282/465, d1=0.729, d2=0.642 g=0.688\n",
            ">1, 283/465, d1=0.460, d2=0.642 g=1.161\n",
            ">1, 284/465, d1=0.716, d2=0.556 g=1.258\n",
            ">1, 285/465, d1=0.893, d2=0.595 g=1.081\n",
            ">1, 286/465, d1=0.669, d2=0.619 g=0.974\n",
            ">1, 287/465, d1=0.761, d2=0.691 g=1.009\n",
            ">1, 288/465, d1=0.620, d2=0.647 g=1.275\n",
            ">1, 289/465, d1=0.596, d2=0.643 g=1.065\n",
            ">1, 290/465, d1=0.915, d2=0.696 g=1.659\n",
            ">1, 291/465, d1=0.712, d2=0.471 g=2.005\n",
            ">1, 292/465, d1=0.727, d2=0.432 g=1.607\n",
            ">1, 293/465, d1=0.558, d2=0.754 g=1.251\n",
            ">1, 294/465, d1=0.772, d2=0.794 g=1.066\n",
            ">1, 295/465, d1=0.918, d2=0.749 g=1.109\n",
            ">1, 296/465, d1=0.649, d2=0.642 g=1.160\n",
            ">1, 297/465, d1=0.690, d2=0.680 g=1.033\n",
            ">1, 298/465, d1=0.817, d2=0.731 g=1.087\n",
            ">1, 299/465, d1=0.958, d2=0.645 g=0.799\n",
            ">1, 300/465, d1=0.490, d2=0.709 g=1.175\n",
            ">1, 301/465, d1=1.002, d2=0.573 g=0.921\n",
            ">1, 302/465, d1=0.695, d2=0.646 g=1.093\n",
            ">1, 303/465, d1=0.513, d2=0.576 g=1.224\n",
            ">1, 304/465, d1=0.646, d2=0.678 g=1.267\n",
            ">1, 305/465, d1=0.535, d2=0.647 g=1.169\n",
            ">1, 306/465, d1=0.808, d2=0.686 g=1.169\n",
            ">1, 307/465, d1=0.735, d2=0.946 g=1.140\n",
            ">1, 308/465, d1=0.746, d2=0.750 g=1.017\n",
            ">1, 309/465, d1=0.713, d2=0.687 g=1.138\n",
            ">1, 310/465, d1=0.639, d2=0.677 g=1.266\n",
            ">1, 311/465, d1=0.748, d2=0.536 g=1.227\n",
            ">1, 312/465, d1=0.696, d2=0.674 g=1.033\n",
            ">1, 313/465, d1=0.509, d2=0.609 g=1.486\n",
            ">1, 314/465, d1=0.756, d2=0.526 g=1.067\n",
            ">1, 315/465, d1=0.697, d2=0.637 g=0.983\n",
            ">1, 316/465, d1=0.514, d2=0.754 g=1.039\n",
            ">1, 317/465, d1=1.569, d2=0.627 g=1.545\n",
            ">1, 318/465, d1=0.898, d2=0.626 g=1.030\n",
            ">1, 319/465, d1=0.620, d2=0.710 g=0.962\n",
            ">1, 320/465, d1=0.628, d2=0.618 g=1.258\n",
            ">1, 321/465, d1=1.309, d2=0.738 g=0.802\n",
            ">1, 322/465, d1=0.592, d2=0.551 g=1.128\n",
            ">1, 323/465, d1=0.832, d2=0.756 g=1.408\n",
            ">1, 324/465, d1=0.788, d2=0.667 g=0.946\n",
            ">1, 325/465, d1=0.581, d2=0.661 g=1.079\n",
            ">1, 326/465, d1=0.952, d2=0.590 g=0.855\n",
            ">1, 327/465, d1=0.823, d2=0.647 g=0.872\n",
            ">1, 328/465, d1=0.643, d2=0.660 g=0.938\n",
            ">1, 329/465, d1=0.614, d2=0.620 g=0.984\n",
            ">1, 330/465, d1=0.592, d2=0.662 g=1.103\n",
            ">1, 331/465, d1=0.733, d2=0.698 g=0.947\n",
            ">1, 332/465, d1=0.784, d2=0.603 g=0.932\n",
            ">1, 333/465, d1=0.675, d2=0.650 g=1.030\n",
            ">1, 334/465, d1=0.698, d2=0.591 g=1.102\n",
            ">1, 335/465, d1=1.227, d2=0.578 g=0.946\n",
            ">1, 336/465, d1=1.000, d2=0.784 g=0.806\n",
            ">1, 337/465, d1=0.614, d2=0.753 g=0.789\n",
            ">1, 338/465, d1=0.563, d2=0.738 g=0.954\n",
            ">1, 339/465, d1=0.661, d2=0.640 g=1.053\n",
            ">1, 340/465, d1=0.673, d2=0.716 g=1.155\n",
            ">1, 341/465, d1=0.667, d2=0.620 g=1.012\n",
            ">1, 342/465, d1=0.893, d2=0.630 g=0.926\n",
            ">1, 343/465, d1=0.463, d2=0.624 g=1.067\n",
            ">1, 344/465, d1=0.610, d2=0.626 g=1.245\n",
            ">1, 345/465, d1=0.546, d2=0.554 g=1.125\n",
            ">1, 346/465, d1=0.729, d2=0.490 g=1.107\n",
            ">1, 347/465, d1=0.613, d2=0.683 g=1.041\n",
            ">1, 348/465, d1=0.560, d2=0.594 g=1.013\n",
            ">1, 349/465, d1=0.635, d2=0.773 g=1.181\n",
            ">1, 350/465, d1=0.782, d2=0.659 g=1.121\n",
            ">1, 351/465, d1=0.613, d2=0.583 g=1.207\n",
            ">1, 352/465, d1=0.837, d2=0.880 g=1.214\n",
            ">1, 353/465, d1=0.800, d2=0.657 g=1.276\n",
            ">1, 354/465, d1=0.631, d2=0.714 g=0.879\n",
            ">1, 355/465, d1=0.493, d2=1.266 g=1.028\n",
            ">1, 356/465, d1=0.568, d2=0.608 g=1.043\n",
            ">1, 357/465, d1=0.529, d2=0.592 g=1.215\n",
            ">1, 358/465, d1=0.615, d2=0.637 g=1.057\n",
            ">1, 359/465, d1=0.755, d2=0.671 g=1.123\n",
            ">1, 360/465, d1=0.654, d2=0.627 g=1.111\n",
            ">1, 361/465, d1=0.478, d2=0.579 g=1.041\n",
            ">1, 362/465, d1=0.619, d2=0.602 g=1.063\n",
            ">1, 363/465, d1=0.805, d2=0.600 g=1.450\n",
            ">1, 364/465, d1=0.612, d2=0.557 g=1.120\n",
            ">1, 365/465, d1=0.594, d2=0.692 g=1.168\n",
            ">1, 366/465, d1=0.854, d2=0.686 g=1.303\n",
            ">1, 367/465, d1=0.929, d2=0.722 g=1.024\n",
            ">1, 368/465, d1=0.820, d2=0.631 g=0.894\n",
            ">1, 369/465, d1=0.808, d2=0.719 g=0.894\n",
            ">1, 370/465, d1=0.645, d2=0.676 g=1.177\n",
            ">1, 371/465, d1=0.760, d2=0.724 g=0.980\n",
            ">1, 372/465, d1=0.618, d2=0.589 g=0.945\n",
            ">1, 373/465, d1=0.798, d2=0.597 g=1.004\n",
            ">1, 374/465, d1=0.770, d2=0.596 g=0.942\n",
            ">1, 375/465, d1=0.628, d2=0.711 g=1.007\n",
            ">1, 376/465, d1=0.729, d2=0.689 g=1.040\n",
            ">1, 377/465, d1=0.626, d2=0.631 g=1.011\n",
            ">1, 378/465, d1=0.730, d2=0.609 g=1.053\n",
            ">1, 379/465, d1=0.460, d2=0.714 g=1.237\n",
            ">1, 380/465, d1=0.572, d2=0.999 g=1.195\n",
            ">1, 381/465, d1=0.609, d2=0.555 g=1.264\n",
            ">1, 382/465, d1=0.615, d2=0.639 g=1.419\n",
            ">1, 383/465, d1=0.601, d2=0.691 g=1.454\n",
            ">1, 384/465, d1=1.057, d2=0.684 g=1.191\n",
            ">1, 385/465, d1=0.703, d2=0.720 g=1.116\n",
            ">1, 386/465, d1=0.834, d2=0.599 g=1.010\n",
            ">1, 387/465, d1=0.622, d2=0.567 g=1.021\n",
            ">1, 388/465, d1=0.712, d2=0.705 g=1.020\n",
            ">1, 389/465, d1=0.491, d2=0.676 g=1.262\n",
            ">1, 390/465, d1=0.631, d2=0.655 g=1.194\n",
            ">1, 391/465, d1=0.558, d2=0.733 g=1.121\n",
            ">1, 392/465, d1=0.754, d2=0.568 g=1.313\n",
            ">1, 393/465, d1=0.802, d2=0.503 g=1.261\n",
            ">1, 394/465, d1=0.714, d2=0.626 g=1.265\n",
            ">1, 395/465, d1=0.571, d2=0.733 g=1.172\n",
            ">1, 396/465, d1=1.149, d2=0.713 g=1.000\n",
            ">1, 397/465, d1=0.692, d2=0.639 g=1.145\n",
            ">1, 398/465, d1=0.502, d2=0.550 g=1.285\n",
            ">1, 399/465, d1=0.592, d2=0.625 g=1.244\n",
            ">1, 400/465, d1=0.374, d2=0.634 g=1.344\n",
            ">1, 401/465, d1=1.244, d2=0.648 g=1.296\n",
            ">1, 402/465, d1=0.595, d2=0.762 g=0.917\n",
            ">1, 403/465, d1=0.786, d2=0.800 g=1.121\n",
            ">1, 404/465, d1=0.637, d2=0.626 g=1.283\n",
            ">1, 405/465, d1=0.558, d2=0.642 g=1.387\n",
            ">1, 406/465, d1=0.641, d2=0.658 g=1.266\n",
            ">1, 407/465, d1=0.813, d2=0.669 g=1.213\n",
            ">1, 408/465, d1=0.393, d2=0.779 g=1.585\n",
            ">1, 409/465, d1=0.744, d2=0.473 g=1.599\n",
            ">1, 410/465, d1=1.080, d2=0.712 g=1.126\n",
            ">1, 411/465, d1=0.669, d2=0.675 g=1.141\n",
            ">1, 412/465, d1=0.546, d2=0.535 g=1.304\n",
            ">1, 413/465, d1=1.231, d2=0.723 g=1.098\n",
            ">1, 414/465, d1=0.635, d2=0.823 g=0.991\n",
            ">1, 415/465, d1=0.740, d2=0.684 g=1.417\n",
            ">1, 416/465, d1=0.711, d2=0.687 g=1.352\n",
            ">1, 417/465, d1=0.511, d2=0.615 g=1.103\n",
            ">1, 418/465, d1=0.985, d2=0.556 g=0.980\n",
            ">1, 419/465, d1=0.802, d2=0.879 g=1.016\n",
            ">1, 420/465, d1=0.708, d2=0.786 g=1.303\n",
            ">1, 421/465, d1=0.911, d2=0.684 g=1.137\n",
            ">1, 422/465, d1=0.637, d2=0.618 g=1.530\n",
            ">1, 423/465, d1=0.497, d2=0.585 g=2.048\n",
            ">1, 424/465, d1=0.686, d2=0.592 g=1.621\n",
            ">1, 425/465, d1=0.611, d2=0.642 g=1.505\n",
            ">1, 426/465, d1=0.559, d2=0.602 g=1.258\n",
            ">1, 427/465, d1=0.729, d2=0.726 g=1.409\n",
            ">1, 428/465, d1=0.730, d2=0.586 g=1.171\n",
            ">1, 429/465, d1=0.987, d2=0.703 g=1.081\n",
            ">1, 430/465, d1=0.652, d2=0.797 g=1.533\n",
            ">1, 431/465, d1=0.882, d2=0.575 g=0.976\n",
            ">1, 432/465, d1=0.542, d2=0.565 g=1.203\n",
            ">1, 433/465, d1=1.243, d2=0.805 g=1.030\n",
            ">1, 434/465, d1=0.656, d2=0.820 g=1.121\n",
            ">1, 435/465, d1=0.588, d2=0.589 g=1.198\n",
            ">1, 436/465, d1=0.567, d2=0.623 g=1.134\n",
            ">1, 437/465, d1=0.720, d2=0.639 g=1.334\n",
            ">1, 438/465, d1=0.617, d2=0.671 g=1.176\n",
            ">1, 439/465, d1=0.547, d2=0.577 g=1.310\n",
            ">1, 440/465, d1=0.655, d2=0.641 g=1.400\n",
            ">1, 441/465, d1=0.579, d2=0.607 g=1.243\n",
            ">1, 442/465, d1=0.686, d2=0.575 g=0.926\n",
            ">1, 443/465, d1=1.191, d2=0.894 g=0.964\n",
            ">1, 444/465, d1=0.742, d2=0.554 g=0.958\n",
            ">1, 445/465, d1=0.476, d2=0.696 g=1.237\n",
            ">1, 446/465, d1=0.594, d2=0.570 g=0.877\n",
            ">1, 447/465, d1=0.599, d2=0.570 g=1.128\n",
            ">1, 448/465, d1=0.762, d2=0.645 g=1.300\n",
            ">1, 449/465, d1=0.585, d2=0.722 g=1.171\n",
            ">1, 450/465, d1=0.662, d2=0.649 g=0.982\n",
            ">1, 451/465, d1=0.615, d2=0.752 g=0.991\n",
            ">1, 452/465, d1=0.569, d2=0.658 g=1.182\n",
            ">1, 453/465, d1=1.301, d2=0.702 g=1.141\n",
            ">1, 454/465, d1=0.509, d2=0.676 g=1.166\n",
            ">1, 455/465, d1=0.483, d2=0.544 g=1.282\n",
            ">1, 456/465, d1=0.766, d2=0.691 g=1.144\n",
            ">1, 457/465, d1=0.641, d2=0.646 g=1.078\n",
            ">1, 458/465, d1=0.530, d2=0.593 g=0.992\n",
            ">1, 459/465, d1=1.034, d2=0.930 g=1.166\n",
            ">1, 460/465, d1=0.608, d2=0.505 g=0.981\n",
            ">1, 461/465, d1=0.760, d2=0.688 g=1.142\n",
            ">1, 462/465, d1=0.668, d2=0.732 g=0.937\n",
            ">1, 463/465, d1=0.627, d2=0.674 g=1.318\n",
            ">1, 464/465, d1=0.821, d2=0.656 g=1.273\n",
            ">1, 465/465, d1=0.612, d2=0.683 g=1.125\n",
            ">2, 1/465, d1=0.761, d2=0.799 g=1.057\n",
            ">2, 2/465, d1=0.629, d2=0.754 g=1.111\n",
            ">2, 3/465, d1=0.484, d2=0.757 g=1.580\n",
            ">2, 4/465, d1=0.839, d2=0.571 g=1.027\n",
            ">2, 5/465, d1=0.957, d2=0.544 g=1.033\n",
            ">2, 6/465, d1=0.725, d2=0.751 g=1.055\n",
            ">2, 7/465, d1=0.691, d2=0.786 g=0.911\n",
            ">2, 8/465, d1=0.673, d2=0.580 g=1.216\n",
            ">2, 9/465, d1=0.942, d2=0.743 g=0.859\n",
            ">2, 10/465, d1=0.766, d2=0.659 g=0.923\n",
            ">2, 11/465, d1=0.751, d2=0.729 g=1.053\n",
            ">2, 12/465, d1=0.615, d2=0.669 g=1.474\n",
            ">2, 13/465, d1=0.908, d2=0.543 g=0.939\n",
            ">2, 14/465, d1=0.584, d2=0.686 g=1.079\n",
            ">2, 15/465, d1=0.638, d2=0.686 g=0.899\n",
            ">2, 16/465, d1=0.442, d2=0.741 g=0.884\n",
            ">2, 17/465, d1=0.536, d2=0.846 g=1.108\n",
            ">2, 18/465, d1=0.565, d2=0.574 g=1.132\n",
            ">2, 19/465, d1=0.981, d2=0.706 g=1.123\n",
            ">2, 20/465, d1=0.586, d2=0.634 g=1.839\n",
            ">2, 21/465, d1=0.484, d2=0.581 g=1.521\n",
            ">2, 22/465, d1=0.415, d2=0.901 g=1.049\n",
            ">2, 23/465, d1=0.526, d2=0.799 g=1.150\n",
            ">2, 24/465, d1=0.764, d2=0.784 g=1.160\n",
            ">2, 25/465, d1=0.560, d2=0.682 g=1.242\n",
            ">2, 26/465, d1=0.546, d2=0.553 g=1.171\n",
            ">2, 27/465, d1=0.496, d2=0.758 g=1.343\n",
            ">2, 28/465, d1=0.820, d2=0.688 g=1.251\n",
            ">2, 29/465, d1=0.570, d2=0.702 g=1.270\n",
            ">2, 30/465, d1=0.622, d2=0.624 g=1.399\n",
            ">2, 31/465, d1=0.737, d2=0.781 g=2.087\n",
            ">2, 32/465, d1=0.746, d2=0.583 g=1.202\n",
            ">2, 33/465, d1=0.701, d2=0.811 g=1.279\n",
            ">2, 34/465, d1=0.726, d2=0.611 g=1.500\n",
            ">2, 35/465, d1=0.569, d2=0.579 g=1.502\n",
            ">2, 36/465, d1=0.683, d2=0.653 g=1.104\n",
            ">2, 37/465, d1=1.024, d2=0.748 g=1.061\n",
            ">2, 38/465, d1=0.500, d2=0.923 g=1.293\n",
            ">2, 39/465, d1=0.504, d2=0.808 g=1.213\n",
            ">2, 40/465, d1=1.035, d2=0.630 g=1.231\n",
            ">2, 41/465, d1=0.779, d2=0.690 g=1.261\n",
            ">2, 42/465, d1=0.632, d2=0.661 g=1.105\n",
            ">2, 43/465, d1=0.743, d2=0.680 g=1.096\n",
            ">2, 44/465, d1=0.708, d2=0.636 g=1.277\n",
            ">2, 45/465, d1=0.529, d2=0.495 g=1.343\n",
            ">2, 46/465, d1=0.799, d2=0.774 g=0.947\n",
            ">2, 47/465, d1=0.462, d2=0.592 g=1.223\n",
            ">2, 48/465, d1=0.506, d2=0.566 g=1.391\n",
            ">2, 49/465, d1=0.587, d2=0.674 g=1.326\n",
            ">2, 50/465, d1=0.631, d2=0.704 g=1.281\n",
            ">2, 51/465, d1=0.747, d2=0.853 g=1.183\n",
            ">2, 52/465, d1=1.315, d2=0.847 g=1.339\n",
            ">2, 53/465, d1=0.906, d2=0.631 g=1.409\n",
            ">2, 54/465, d1=0.766, d2=0.704 g=1.128\n",
            ">2, 55/465, d1=0.869, d2=0.633 g=1.557\n",
            ">2, 56/465, d1=0.643, d2=0.506 g=1.282\n",
            ">2, 57/465, d1=0.487, d2=0.633 g=1.361\n",
            ">2, 58/465, d1=0.547, d2=0.464 g=1.192\n",
            ">2, 59/465, d1=0.661, d2=0.646 g=1.900\n",
            ">2, 60/465, d1=0.678, d2=0.652 g=1.377\n",
            ">2, 61/465, d1=0.710, d2=0.713 g=1.161\n",
            ">2, 62/465, d1=0.881, d2=0.930 g=1.326\n",
            ">2, 63/465, d1=0.885, d2=0.550 g=1.241\n",
            ">2, 64/465, d1=0.537, d2=0.636 g=1.305\n",
            ">2, 65/465, d1=0.631, d2=0.644 g=1.116\n",
            ">2, 66/465, d1=0.752, d2=0.870 g=0.735\n",
            ">2, 67/465, d1=0.730, d2=0.705 g=1.029\n",
            ">2, 68/465, d1=0.717, d2=0.694 g=1.185\n",
            ">2, 69/465, d1=0.657, d2=0.618 g=1.139\n",
            ">2, 70/465, d1=0.826, d2=0.725 g=0.866\n",
            ">2, 71/465, d1=0.626, d2=0.821 g=1.149\n",
            ">2, 72/465, d1=0.513, d2=0.614 g=1.376\n",
            ">2, 73/465, d1=0.591, d2=0.717 g=1.019\n",
            ">2, 74/465, d1=0.936, d2=0.683 g=1.350\n",
            ">2, 75/465, d1=0.663, d2=0.538 g=1.367\n",
            ">2, 76/465, d1=0.509, d2=0.759 g=1.117\n",
            ">2, 77/465, d1=0.803, d2=0.766 g=0.923\n",
            ">2, 78/465, d1=0.609, d2=0.756 g=1.208\n",
            ">2, 79/465, d1=0.665, d2=0.684 g=0.888\n",
            ">2, 80/465, d1=0.693, d2=0.769 g=1.529\n",
            ">2, 81/465, d1=0.839, d2=0.806 g=1.458\n",
            ">2, 82/465, d1=0.865, d2=0.696 g=1.223\n",
            ">2, 83/465, d1=0.560, d2=0.596 g=1.319\n",
            ">2, 84/465, d1=0.597, d2=0.612 g=1.195\n",
            ">2, 85/465, d1=0.701, d2=0.674 g=1.236\n",
            ">2, 86/465, d1=0.632, d2=0.723 g=0.959\n",
            ">2, 87/465, d1=0.539, d2=0.705 g=1.140\n",
            ">2, 88/465, d1=0.643, d2=0.723 g=1.102\n",
            ">2, 89/465, d1=0.617, d2=0.722 g=0.939\n",
            ">2, 90/465, d1=0.688, d2=0.842 g=0.953\n",
            ">2, 91/465, d1=0.766, d2=0.679 g=1.062\n",
            ">2, 92/465, d1=0.626, d2=0.686 g=0.975\n",
            ">2, 93/465, d1=0.740, d2=0.588 g=1.081\n",
            ">2, 94/465, d1=0.627, d2=0.568 g=1.236\n",
            ">2, 95/465, d1=0.760, d2=0.730 g=1.144\n",
            ">2, 96/465, d1=0.725, d2=0.669 g=1.137\n",
            ">2, 97/465, d1=0.968, d2=0.597 g=1.886\n",
            ">2, 98/465, d1=1.079, d2=0.671 g=1.222\n",
            ">2, 99/465, d1=0.636, d2=0.624 g=1.494\n",
            ">2, 100/465, d1=0.556, d2=0.645 g=1.698\n",
            ">2, 101/465, d1=0.806, d2=0.546 g=1.298\n",
            ">2, 102/465, d1=0.765, d2=0.657 g=1.160\n",
            ">2, 103/465, d1=0.494, d2=0.711 g=1.114\n",
            ">2, 104/465, d1=0.787, d2=0.775 g=1.061\n",
            ">2, 105/465, d1=0.813, d2=0.593 g=1.050\n",
            ">2, 106/465, d1=0.642, d2=0.566 g=1.136\n",
            ">2, 107/465, d1=0.610, d2=0.572 g=1.242\n",
            ">2, 108/465, d1=0.768, d2=0.662 g=0.907\n",
            ">2, 109/465, d1=0.692, d2=0.773 g=1.138\n",
            ">2, 110/465, d1=0.881, d2=0.657 g=1.066\n",
            ">2, 111/465, d1=0.739, d2=0.702 g=1.226\n",
            ">2, 112/465, d1=0.737, d2=0.886 g=1.510\n",
            ">2, 113/465, d1=0.728, d2=0.510 g=1.214\n",
            ">2, 114/465, d1=0.720, d2=0.578 g=0.854\n",
            ">2, 115/465, d1=0.574, d2=0.919 g=1.130\n",
            ">2, 116/465, d1=0.730, d2=0.629 g=1.439\n",
            ">2, 117/465, d1=0.658, d2=0.578 g=1.064\n",
            ">2, 118/465, d1=0.859, d2=0.725 g=1.008\n",
            ">2, 119/465, d1=0.510, d2=0.606 g=0.975\n",
            ">2, 120/465, d1=0.452, d2=0.857 g=1.487\n",
            ">2, 121/465, d1=0.845, d2=0.623 g=1.214\n",
            ">2, 122/465, d1=0.785, d2=0.748 g=0.949\n",
            ">2, 123/465, d1=0.639, d2=0.683 g=1.164\n",
            ">2, 124/465, d1=0.661, d2=0.654 g=1.148\n",
            ">2, 125/465, d1=0.517, d2=0.751 g=1.548\n",
            ">2, 126/465, d1=0.906, d2=0.521 g=1.043\n",
            ">2, 127/465, d1=0.684, d2=0.895 g=1.093\n",
            ">2, 128/465, d1=0.749, d2=0.597 g=1.450\n",
            ">2, 129/465, d1=1.062, d2=0.560 g=1.394\n",
            ">2, 130/465, d1=0.751, d2=0.814 g=0.902\n",
            ">2, 131/465, d1=0.644, d2=0.744 g=0.897\n",
            ">2, 132/465, d1=0.636, d2=0.608 g=0.999\n",
            ">2, 133/465, d1=0.856, d2=0.572 g=0.955\n",
            ">2, 134/465, d1=0.709, d2=0.657 g=1.540\n",
            ">2, 135/465, d1=0.617, d2=1.129 g=1.518\n",
            ">2, 136/465, d1=0.724, d2=0.593 g=1.152\n",
            ">2, 137/465, d1=0.772, d2=0.860 g=1.012\n",
            ">2, 138/465, d1=0.875, d2=0.618 g=1.146\n",
            ">2, 139/465, d1=0.598, d2=0.605 g=1.196\n",
            ">2, 140/465, d1=0.820, d2=0.605 g=1.117\n",
            ">2, 141/465, d1=0.796, d2=0.613 g=1.705\n",
            ">2, 142/465, d1=0.971, d2=0.612 g=1.074\n",
            ">2, 143/465, d1=0.835, d2=0.691 g=1.362\n",
            ">2, 144/465, d1=0.638, d2=0.597 g=1.281\n",
            ">2, 145/465, d1=0.591, d2=0.638 g=1.275\n",
            ">2, 146/465, d1=0.748, d2=0.776 g=1.351\n",
            ">2, 147/465, d1=0.818, d2=0.606 g=1.143\n",
            ">2, 148/465, d1=0.837, d2=0.636 g=1.105\n",
            ">2, 149/465, d1=0.716, d2=0.551 g=0.954\n",
            ">2, 150/465, d1=0.643, d2=0.514 g=0.924\n",
            ">2, 151/465, d1=1.026, d2=0.651 g=1.206\n",
            ">2, 152/465, d1=0.862, d2=0.672 g=1.127\n",
            ">2, 153/465, d1=0.697, d2=0.644 g=1.281\n",
            ">2, 154/465, d1=0.847, d2=0.707 g=0.803\n",
            ">2, 155/465, d1=0.713, d2=0.726 g=0.912\n",
            ">2, 156/465, d1=0.564, d2=0.883 g=1.061\n",
            ">2, 157/465, d1=0.618, d2=0.671 g=1.227\n",
            ">2, 158/465, d1=0.713, d2=0.611 g=1.251\n",
            ">2, 159/465, d1=0.793, d2=0.909 g=0.915\n",
            ">2, 160/465, d1=0.552, d2=0.650 g=1.036\n",
            ">2, 161/465, d1=0.678, d2=0.589 g=1.031\n",
            ">2, 162/465, d1=0.705, d2=0.594 g=1.015\n",
            ">2, 163/465, d1=0.601, d2=0.668 g=0.994\n",
            ">2, 164/465, d1=0.941, d2=0.661 g=0.911\n",
            ">2, 165/465, d1=0.754, d2=0.777 g=0.918\n",
            ">2, 166/465, d1=0.742, d2=0.666 g=1.061\n",
            ">2, 167/465, d1=0.662, d2=0.719 g=0.858\n",
            ">2, 168/465, d1=0.744, d2=0.721 g=0.954\n",
            ">2, 169/465, d1=1.109, d2=0.641 g=0.956\n",
            ">2, 170/465, d1=0.634, d2=0.659 g=1.036\n",
            ">2, 171/465, d1=0.675, d2=0.528 g=1.059\n",
            ">2, 172/465, d1=0.583, d2=0.717 g=1.377\n",
            ">2, 173/465, d1=0.752, d2=0.788 g=1.007\n",
            ">2, 174/465, d1=0.964, d2=0.634 g=0.879\n",
            ">2, 175/465, d1=0.788, d2=0.532 g=1.097\n",
            ">2, 176/465, d1=0.594, d2=0.779 g=1.051\n",
            ">2, 177/465, d1=0.730, d2=0.861 g=0.974\n",
            ">2, 178/465, d1=0.622, d2=0.675 g=1.089\n",
            ">2, 179/465, d1=0.682, d2=0.534 g=1.101\n",
            ">2, 180/465, d1=0.678, d2=0.751 g=0.968\n",
            ">2, 181/465, d1=0.656, d2=0.722 g=1.158\n",
            ">2, 182/465, d1=0.674, d2=0.675 g=1.187\n",
            ">2, 183/465, d1=0.827, d2=0.613 g=1.261\n",
            ">2, 184/465, d1=0.712, d2=0.647 g=0.939\n",
            ">2, 185/465, d1=0.566, d2=0.647 g=1.091\n",
            ">2, 186/465, d1=0.866, d2=0.642 g=1.104\n",
            ">2, 187/465, d1=0.632, d2=0.658 g=1.103\n",
            ">2, 188/465, d1=0.713, d2=0.532 g=1.044\n",
            ">2, 189/465, d1=0.732, d2=0.721 g=0.943\n",
            ">2, 190/465, d1=0.671, d2=0.697 g=1.261\n",
            ">2, 191/465, d1=0.628, d2=0.423 g=1.508\n",
            ">2, 192/465, d1=0.317, d2=0.691 g=0.858\n",
            ">2, 193/465, d1=1.106, d2=0.667 g=0.583\n",
            ">2, 194/465, d1=0.627, d2=0.782 g=0.860\n",
            ">2, 195/465, d1=0.624, d2=0.576 g=0.970\n",
            ">2, 196/465, d1=0.762, d2=0.728 g=0.905\n",
            ">2, 197/465, d1=0.718, d2=0.759 g=1.060\n",
            ">2, 198/465, d1=0.698, d2=0.633 g=0.989\n",
            ">2, 199/465, d1=0.738, d2=0.526 g=0.875\n",
            ">2, 200/465, d1=0.714, d2=0.725 g=1.093\n",
            ">2, 201/465, d1=0.642, d2=0.677 g=0.901\n",
            ">2, 202/465, d1=0.760, d2=0.762 g=1.092\n",
            ">2, 203/465, d1=0.895, d2=0.595 g=1.158\n",
            ">2, 204/465, d1=0.546, d2=0.637 g=0.697\n",
            ">2, 205/465, d1=0.881, d2=0.604 g=0.858\n",
            ">2, 206/465, d1=0.709, d2=0.808 g=1.040\n",
            ">2, 207/465, d1=0.763, d2=0.673 g=1.022\n",
            ">2, 208/465, d1=0.846, d2=0.725 g=1.108\n",
            ">2, 209/465, d1=0.538, d2=0.980 g=1.260\n",
            ">2, 210/465, d1=0.529, d2=0.624 g=1.288\n",
            ">2, 211/465, d1=0.582, d2=0.645 g=1.111\n",
            ">2, 212/465, d1=0.850, d2=0.761 g=1.121\n",
            ">2, 213/465, d1=0.833, d2=0.619 g=1.063\n",
            ">2, 214/465, d1=0.693, d2=0.673 g=1.059\n",
            ">2, 215/465, d1=0.576, d2=0.629 g=1.085\n",
            ">2, 216/465, d1=0.838, d2=0.880 g=1.318\n",
            ">2, 217/465, d1=0.841, d2=0.520 g=1.083\n",
            ">2, 218/465, d1=0.612, d2=0.671 g=1.016\n",
            ">2, 219/465, d1=0.695, d2=0.787 g=1.151\n",
            ">2, 220/465, d1=0.689, d2=0.519 g=1.131\n",
            ">2, 221/465, d1=0.529, d2=0.469 g=1.650\n",
            ">2, 222/465, d1=0.833, d2=0.866 g=1.196\n",
            ">2, 223/465, d1=0.510, d2=0.703 g=1.436\n",
            ">2, 224/465, d1=0.613, d2=0.550 g=1.136\n",
            ">2, 225/465, d1=0.550, d2=1.074 g=1.315\n",
            ">2, 226/465, d1=0.843, d2=0.631 g=1.337\n",
            ">2, 227/465, d1=0.732, d2=0.673 g=1.269\n",
            ">2, 228/465, d1=0.655, d2=0.662 g=0.958\n",
            ">2, 229/465, d1=0.810, d2=0.662 g=1.125\n",
            ">2, 230/465, d1=0.607, d2=0.591 g=0.928\n",
            ">2, 231/465, d1=0.968, d2=0.895 g=0.774\n",
            ">2, 232/465, d1=0.636, d2=0.752 g=1.039\n",
            ">2, 233/465, d1=0.724, d2=0.558 g=1.048\n",
            ">2, 234/465, d1=0.757, d2=0.739 g=1.101\n",
            ">2, 235/465, d1=0.588, d2=1.019 g=0.764\n",
            ">2, 236/465, d1=0.680, d2=0.662 g=1.123\n",
            ">2, 237/465, d1=0.484, d2=0.708 g=1.176\n",
            ">2, 238/465, d1=0.905, d2=0.688 g=1.282\n",
            ">2, 239/465, d1=0.676, d2=0.656 g=1.190\n",
            ">2, 240/465, d1=0.684, d2=0.632 g=0.971\n",
            ">2, 241/465, d1=1.208, d2=0.657 g=1.021\n",
            ">2, 242/465, d1=0.665, d2=0.419 g=1.031\n",
            ">2, 243/465, d1=1.092, d2=1.132 g=1.292\n",
            ">2, 244/465, d1=1.061, d2=0.665 g=1.233\n",
            ">2, 245/465, d1=0.585, d2=0.615 g=1.118\n",
            ">2, 246/465, d1=0.800, d2=0.729 g=1.032\n",
            ">2, 247/465, d1=0.614, d2=0.638 g=1.057\n",
            ">2, 248/465, d1=0.709, d2=0.616 g=1.318\n",
            ">2, 249/465, d1=0.874, d2=0.644 g=0.900\n",
            ">2, 250/465, d1=0.643, d2=0.604 g=0.916\n",
            ">2, 251/465, d1=0.401, d2=0.571 g=1.542\n",
            ">2, 252/465, d1=0.736, d2=0.751 g=1.208\n",
            ">2, 253/465, d1=0.800, d2=0.607 g=0.981\n",
            ">2, 254/465, d1=0.576, d2=0.755 g=1.531\n",
            ">2, 255/465, d1=0.860, d2=0.563 g=1.058\n",
            ">2, 256/465, d1=0.580, d2=0.624 g=1.205\n",
            ">2, 257/465, d1=0.650, d2=0.752 g=1.051\n",
            ">2, 258/465, d1=0.882, d2=0.799 g=0.748\n",
            ">2, 259/465, d1=0.557, d2=0.807 g=0.723\n",
            ">2, 260/465, d1=0.739, d2=0.800 g=0.991\n",
            ">2, 261/465, d1=0.941, d2=0.854 g=0.928\n",
            ">2, 262/465, d1=0.903, d2=0.574 g=0.943\n",
            ">2, 263/465, d1=0.742, d2=0.724 g=1.177\n",
            ">2, 264/465, d1=0.784, d2=0.845 g=1.162\n",
            ">2, 265/465, d1=0.716, d2=0.686 g=1.073\n",
            ">2, 266/465, d1=0.670, d2=0.590 g=0.926\n",
            ">2, 267/465, d1=0.615, d2=0.570 g=1.045\n",
            ">2, 268/465, d1=0.670, d2=0.677 g=1.029\n",
            ">2, 269/465, d1=0.564, d2=0.547 g=1.165\n",
            ">2, 270/465, d1=0.700, d2=0.679 g=0.864\n",
            ">2, 271/465, d1=0.655, d2=0.771 g=0.941\n",
            ">2, 272/465, d1=0.557, d2=0.834 g=1.027\n",
            ">2, 273/465, d1=0.911, d2=0.570 g=0.857\n",
            ">2, 274/465, d1=0.651, d2=0.680 g=0.999\n",
            ">2, 275/465, d1=0.868, d2=0.559 g=1.289\n",
            ">2, 276/465, d1=0.610, d2=0.719 g=1.252\n",
            ">2, 277/465, d1=0.472, d2=0.644 g=1.236\n",
            ">2, 278/465, d1=1.294, d2=0.460 g=1.119\n",
            ">2, 279/465, d1=0.866, d2=0.816 g=0.905\n",
            ">2, 280/465, d1=0.658, d2=0.629 g=0.992\n",
            ">2, 281/465, d1=0.585, d2=0.639 g=1.046\n",
            ">2, 282/465, d1=0.672, d2=0.788 g=1.026\n",
            ">2, 283/465, d1=0.536, d2=0.709 g=0.902\n",
            ">2, 284/465, d1=0.673, d2=0.681 g=1.169\n",
            ">2, 285/465, d1=0.802, d2=0.769 g=1.096\n",
            ">2, 286/465, d1=0.709, d2=0.456 g=1.269\n",
            ">2, 287/465, d1=0.621, d2=0.757 g=1.460\n",
            ">2, 288/465, d1=0.691, d2=0.584 g=0.925\n",
            ">2, 289/465, d1=0.680, d2=0.887 g=1.242\n",
            ">2, 290/465, d1=0.724, d2=0.763 g=1.325\n",
            ">2, 291/465, d1=0.849, d2=0.852 g=1.043\n",
            ">2, 292/465, d1=0.560, d2=0.560 g=1.005\n",
            ">2, 293/465, d1=0.715, d2=0.814 g=1.010\n",
            ">2, 294/465, d1=0.867, d2=1.016 g=1.070\n",
            ">2, 295/465, d1=1.108, d2=0.784 g=1.188\n",
            ">2, 296/465, d1=0.733, d2=0.676 g=0.872\n",
            ">2, 297/465, d1=0.511, d2=0.839 g=0.986\n",
            ">2, 298/465, d1=0.521, d2=0.691 g=1.249\n",
            ">2, 299/465, d1=0.754, d2=0.672 g=1.140\n",
            ">2, 300/465, d1=0.877, d2=0.613 g=0.883\n",
            ">2, 301/465, d1=0.631, d2=0.747 g=0.968\n",
            ">2, 302/465, d1=0.735, d2=0.757 g=1.120\n",
            ">2, 303/465, d1=0.708, d2=0.592 g=1.012\n",
            ">2, 304/465, d1=0.466, d2=0.674 g=1.138\n",
            ">2, 305/465, d1=1.203, d2=0.580 g=1.265\n",
            ">2, 306/465, d1=0.571, d2=0.526 g=1.079\n",
            ">2, 307/465, d1=0.903, d2=0.755 g=0.694\n",
            ">2, 308/465, d1=0.663, d2=0.598 g=0.946\n",
            ">2, 309/465, d1=0.717, d2=0.713 g=0.775\n",
            ">2, 310/465, d1=0.990, d2=0.576 g=1.457\n",
            ">2, 311/465, d1=0.755, d2=0.854 g=0.923\n",
            ">2, 312/465, d1=0.844, d2=0.849 g=0.846\n",
            ">2, 313/465, d1=0.607, d2=0.681 g=1.101\n",
            ">2, 314/465, d1=0.672, d2=0.670 g=0.988\n",
            ">2, 315/465, d1=0.785, d2=0.844 g=0.839\n",
            ">2, 316/465, d1=0.687, d2=0.765 g=0.866\n",
            ">2, 317/465, d1=0.590, d2=0.544 g=1.082\n",
            ">2, 318/465, d1=0.587, d2=0.742 g=1.086\n",
            ">2, 319/465, d1=0.550, d2=0.671 g=0.882\n",
            ">2, 320/465, d1=0.646, d2=0.749 g=0.807\n",
            ">2, 321/465, d1=0.712, d2=0.740 g=1.043\n",
            ">2, 322/465, d1=0.694, d2=0.727 g=0.895\n",
            ">2, 323/465, d1=0.712, d2=0.640 g=0.878\n",
            ">2, 324/465, d1=0.878, d2=0.742 g=0.966\n",
            ">2, 325/465, d1=0.390, d2=0.722 g=1.013\n",
            ">2, 326/465, d1=0.744, d2=0.495 g=1.197\n",
            ">2, 327/465, d1=0.644, d2=0.488 g=1.077\n",
            ">2, 328/465, d1=0.626, d2=0.695 g=1.078\n",
            ">2, 329/465, d1=0.679, d2=0.849 g=0.941\n",
            ">2, 330/465, d1=0.545, d2=0.667 g=1.030\n",
            ">2, 331/465, d1=0.546, d2=0.736 g=0.861\n",
            ">2, 332/465, d1=0.650, d2=0.658 g=0.840\n",
            ">2, 333/465, d1=0.729, d2=0.622 g=1.155\n",
            ">2, 334/465, d1=0.686, d2=0.637 g=1.113\n",
            ">2, 335/465, d1=0.578, d2=0.619 g=0.769\n",
            ">2, 336/465, d1=0.616, d2=0.896 g=0.895\n",
            ">2, 337/465, d1=0.519, d2=0.572 g=1.170\n",
            ">2, 338/465, d1=0.882, d2=0.720 g=0.975\n",
            ">2, 339/465, d1=0.826, d2=0.703 g=1.173\n",
            ">2, 340/465, d1=0.591, d2=0.748 g=1.173\n",
            ">2, 341/465, d1=0.621, d2=0.606 g=1.017\n",
            ">2, 342/465, d1=0.448, d2=0.564 g=1.125\n",
            ">2, 343/465, d1=0.770, d2=0.854 g=1.192\n",
            ">2, 344/465, d1=1.037, d2=0.822 g=1.415\n",
            ">2, 345/465, d1=0.899, d2=0.669 g=0.950\n",
            ">2, 346/465, d1=0.687, d2=0.803 g=0.802\n",
            ">2, 347/465, d1=0.425, d2=0.840 g=1.350\n",
            ">2, 348/465, d1=0.571, d2=0.927 g=0.959\n",
            ">2, 349/465, d1=0.493, d2=0.859 g=1.066\n",
            ">2, 350/465, d1=0.908, d2=0.619 g=1.139\n",
            ">2, 351/465, d1=0.812, d2=0.658 g=1.127\n",
            ">2, 352/465, d1=0.829, d2=0.744 g=1.163\n",
            ">2, 353/465, d1=0.639, d2=0.500 g=1.239\n",
            ">2, 354/465, d1=0.796, d2=0.707 g=1.372\n",
            ">2, 355/465, d1=0.646, d2=0.623 g=1.196\n",
            ">2, 356/465, d1=0.536, d2=0.529 g=1.187\n",
            ">2, 357/465, d1=0.556, d2=0.580 g=1.158\n",
            ">2, 358/465, d1=0.533, d2=0.606 g=1.564\n",
            ">2, 359/465, d1=0.836, d2=0.798 g=1.371\n",
            ">2, 360/465, d1=0.695, d2=0.624 g=0.918\n",
            ">2, 361/465, d1=0.557, d2=0.546 g=1.220\n",
            ">2, 362/465, d1=1.054, d2=0.509 g=1.072\n",
            ">2, 363/465, d1=1.194, d2=0.676 g=1.004\n",
            ">2, 364/465, d1=0.784, d2=0.915 g=1.134\n",
            ">2, 365/465, d1=0.662, d2=0.539 g=1.426\n",
            ">2, 366/465, d1=0.876, d2=0.702 g=1.274\n",
            ">2, 367/465, d1=0.742, d2=0.691 g=0.965\n",
            ">2, 368/465, d1=0.853, d2=0.548 g=1.041\n",
            ">2, 369/465, d1=0.641, d2=0.633 g=0.917\n",
            ">2, 370/465, d1=0.457, d2=0.960 g=1.309\n",
            ">2, 371/465, d1=0.884, d2=0.819 g=1.072\n",
            ">2, 372/465, d1=0.568, d2=0.712 g=0.997\n",
            ">2, 373/465, d1=1.074, d2=0.610 g=1.125\n",
            ">2, 374/465, d1=0.741, d2=0.744 g=1.182\n",
            ">2, 375/465, d1=0.725, d2=0.613 g=1.195\n",
            ">2, 376/465, d1=0.611, d2=0.676 g=1.073\n",
            ">2, 377/465, d1=0.608, d2=0.668 g=1.182\n",
            ">2, 378/465, d1=0.624, d2=0.535 g=1.199\n",
            ">2, 379/465, d1=0.971, d2=0.639 g=1.273\n",
            ">2, 380/465, d1=0.781, d2=0.792 g=1.037\n",
            ">2, 381/465, d1=0.871, d2=0.616 g=1.232\n",
            ">2, 382/465, d1=0.708, d2=0.635 g=0.862\n",
            ">2, 383/465, d1=0.858, d2=0.783 g=0.953\n",
            ">2, 384/465, d1=0.695, d2=0.560 g=1.012\n",
            ">2, 385/465, d1=0.768, d2=0.603 g=0.949\n",
            ">2, 386/465, d1=0.712, d2=0.606 g=0.980\n",
            ">2, 387/465, d1=0.715, d2=0.631 g=1.029\n",
            ">2, 388/465, d1=0.774, d2=0.762 g=1.191\n",
            ">2, 389/465, d1=0.729, d2=0.639 g=0.982\n",
            ">2, 390/465, d1=0.527, d2=0.639 g=1.186\n",
            ">2, 391/465, d1=0.571, d2=0.571 g=1.222\n",
            ">2, 392/465, d1=0.721, d2=1.205 g=0.733\n",
            ">2, 393/465, d1=0.818, d2=0.599 g=1.514\n",
            ">2, 394/465, d1=1.274, d2=0.708 g=1.125\n",
            ">2, 395/465, d1=0.785, d2=0.659 g=0.937\n",
            ">2, 396/465, d1=0.658, d2=0.815 g=0.992\n",
            ">2, 397/465, d1=0.764, d2=0.586 g=1.131\n",
            ">2, 398/465, d1=0.869, d2=0.680 g=0.994\n",
            ">2, 399/465, d1=0.603, d2=0.641 g=1.223\n",
            ">2, 400/465, d1=0.875, d2=0.624 g=1.123\n",
            ">2, 401/465, d1=0.982, d2=0.777 g=0.856\n",
            ">2, 402/465, d1=0.665, d2=0.793 g=1.003\n",
            ">2, 403/465, d1=0.730, d2=0.625 g=1.104\n",
            ">2, 404/465, d1=0.857, d2=0.711 g=1.110\n",
            ">2, 405/465, d1=0.800, d2=0.659 g=1.182\n",
            ">2, 406/465, d1=0.690, d2=0.688 g=1.103\n",
            ">2, 407/465, d1=0.513, d2=0.617 g=1.422\n",
            ">2, 408/465, d1=0.685, d2=0.814 g=0.906\n",
            ">2, 409/465, d1=0.636, d2=0.687 g=1.074\n",
            ">2, 410/465, d1=0.671, d2=0.629 g=1.165\n",
            ">2, 411/465, d1=0.760, d2=0.803 g=1.211\n",
            ">2, 412/465, d1=0.595, d2=0.617 g=1.137\n",
            ">2, 413/465, d1=0.586, d2=0.780 g=1.101\n",
            ">2, 414/465, d1=0.739, d2=0.730 g=1.062\n",
            ">2, 415/465, d1=0.830, d2=0.640 g=1.487\n",
            ">2, 416/465, d1=0.868, d2=0.574 g=0.937\n",
            ">2, 417/465, d1=0.858, d2=0.870 g=1.018\n",
            ">2, 418/465, d1=0.670, d2=0.590 g=1.344\n",
            ">2, 419/465, d1=0.823, d2=0.703 g=0.786\n",
            ">2, 420/465, d1=0.790, d2=0.737 g=0.770\n",
            ">2, 421/465, d1=0.625, d2=0.760 g=1.013\n",
            ">2, 422/465, d1=0.734, d2=0.650 g=1.029\n",
            ">2, 423/465, d1=0.768, d2=0.626 g=0.904\n",
            ">2, 424/465, d1=0.579, d2=0.628 g=0.917\n",
            ">2, 425/465, d1=0.706, d2=0.623 g=0.943\n",
            ">2, 426/465, d1=0.582, d2=0.684 g=1.114\n",
            ">2, 427/465, d1=0.814, d2=0.600 g=0.934\n",
            ">2, 428/465, d1=0.622, d2=0.651 g=0.879\n",
            ">2, 429/465, d1=0.595, d2=0.682 g=0.945\n",
            ">2, 430/465, d1=0.641, d2=0.658 g=1.006\n",
            ">2, 431/465, d1=0.823, d2=0.681 g=0.962\n",
            ">2, 432/465, d1=0.669, d2=0.777 g=1.173\n",
            ">2, 433/465, d1=0.703, d2=0.639 g=1.002\n",
            ">2, 434/465, d1=0.658, d2=0.698 g=1.197\n",
            ">2, 435/465, d1=0.507, d2=0.727 g=1.162\n",
            ">2, 436/465, d1=0.843, d2=0.779 g=1.045\n",
            ">2, 437/465, d1=0.706, d2=0.616 g=1.045\n",
            ">2, 438/465, d1=0.878, d2=0.669 g=1.036\n",
            ">2, 439/465, d1=0.602, d2=0.701 g=0.930\n",
            ">2, 440/465, d1=0.869, d2=0.711 g=0.827\n",
            ">2, 441/465, d1=0.754, d2=0.531 g=1.008\n",
            ">2, 442/465, d1=0.780, d2=0.746 g=0.838\n",
            ">2, 443/465, d1=0.624, d2=0.833 g=0.965\n",
            ">2, 444/465, d1=0.759, d2=0.673 g=1.059\n",
            ">2, 445/465, d1=0.819, d2=0.704 g=1.130\n",
            ">2, 446/465, d1=0.663, d2=0.686 g=1.117\n",
            ">2, 447/465, d1=0.663, d2=0.630 g=1.159\n",
            ">2, 448/465, d1=0.718, d2=0.638 g=1.097\n",
            ">2, 449/465, d1=0.608, d2=0.602 g=1.115\n",
            ">2, 450/465, d1=0.871, d2=0.685 g=1.172\n",
            ">2, 451/465, d1=0.803, d2=0.616 g=1.058\n",
            ">2, 452/465, d1=0.611, d2=0.629 g=1.043\n",
            ">2, 453/465, d1=0.532, d2=0.734 g=1.021\n",
            ">2, 454/465, d1=0.697, d2=1.027 g=1.188\n",
            ">2, 455/465, d1=0.907, d2=0.714 g=1.065\n",
            ">2, 456/465, d1=0.690, d2=0.813 g=1.066\n",
            ">2, 457/465, d1=0.698, d2=0.675 g=0.981\n",
            ">2, 458/465, d1=0.693, d2=0.650 g=0.941\n",
            ">2, 459/465, d1=0.764, d2=0.714 g=1.078\n",
            ">2, 460/465, d1=0.711, d2=0.878 g=0.994\n",
            ">2, 461/465, d1=0.663, d2=0.654 g=0.935\n",
            ">2, 462/465, d1=0.711, d2=0.935 g=1.027\n",
            ">2, 463/465, d1=0.806, d2=0.779 g=0.984\n",
            ">2, 464/465, d1=0.723, d2=0.554 g=0.972\n",
            ">2, 465/465, d1=0.722, d2=0.793 g=1.062\n",
            ">3, 1/465, d1=0.780, d2=0.574 g=1.046\n",
            ">3, 2/465, d1=0.851, d2=0.874 g=0.947\n",
            ">3, 3/465, d1=0.827, d2=0.877 g=0.809\n",
            ">3, 4/465, d1=0.707, d2=0.741 g=0.984\n",
            ">3, 5/465, d1=0.777, d2=0.643 g=0.953\n",
            ">3, 6/465, d1=0.701, d2=0.671 g=1.035\n",
            ">3, 7/465, d1=0.624, d2=0.600 g=0.930\n",
            ">3, 8/465, d1=0.814, d2=0.540 g=0.922\n",
            ">3, 9/465, d1=0.652, d2=0.873 g=0.950\n",
            ">3, 10/465, d1=0.726, d2=0.656 g=1.151\n",
            ">3, 11/465, d1=1.074, d2=0.622 g=1.346\n",
            ">3, 12/465, d1=0.749, d2=0.643 g=1.143\n",
            ">3, 13/465, d1=0.677, d2=0.665 g=0.839\n",
            ">3, 14/465, d1=0.937, d2=0.911 g=0.833\n",
            ">3, 15/465, d1=0.483, d2=0.673 g=1.050\n",
            ">3, 16/465, d1=0.775, d2=0.634 g=1.124\n",
            ">3, 17/465, d1=0.886, d2=0.675 g=0.990\n",
            ">3, 18/465, d1=0.887, d2=0.930 g=0.792\n",
            ">3, 19/465, d1=0.573, d2=0.641 g=0.956\n",
            ">3, 20/465, d1=0.789, d2=0.652 g=0.929\n",
            ">3, 21/465, d1=0.561, d2=0.683 g=0.856\n",
            ">3, 22/465, d1=0.838, d2=0.684 g=0.827\n",
            ">3, 23/465, d1=0.639, d2=0.550 g=0.889\n",
            ">3, 24/465, d1=0.599, d2=0.777 g=1.048\n",
            ">3, 25/465, d1=0.715, d2=0.707 g=1.005\n"
          ]
        }
      ],
      "execution_count": 18,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1620402030386
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g_model.save(os.path.join(DATA_PATH, f'generator_{image_size}_100_new.h5'))"
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1620768319728
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}